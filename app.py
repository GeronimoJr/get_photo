import streamlit as st
import requests
import tempfile
import os
import re
import json
import time
import random
from datetime import datetime
import pandas as pd
import io
import ftplib
import xml.etree.ElementTree as ET
from urllib.parse import urlparse
import uuid
import codecs
from bs4 import BeautifulSoup
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from oauth2client.service_account import ServiceAccountCredentials
import concurrent.futures
import queue
import threading
import traceback
from concurrent.futures import ThreadPoolExecutor

# Sta≈Çe konfiguracyjne
FTP_CONFIG = {
    'MIN_CONNECTIONS': 1,
    'MAX_CONNECTIONS': 5,
    'MIN_BATCH_SIZE': 2,
    'MAX_BATCH_SIZE': 10,
    'DEFAULT_RETRY_DELAY': 1,
    'MAX_RETRY_DELAY': 5,
    'CONNECTION_TIMEOUT': 60,
    'MIN_WORKERS': 1,
    'MAX_WORKERS': 10,
    'DEFAULT_WORKERS': 3,
    'KEEPALIVE_INTERVAL': 30,  # Interwa≈Ç w sekundach dla keepalive
    'IDLE_TIMEOUT': 300  # Timeout bezczynno≈õci w sekundach (5 minut)
}

class FTPManager:
    _instances = {}
    _lock = threading.Lock()
    
    @classmethod
    def get_instance(cls, settings):
        key = f"{settings['host']}:{settings['username']}:{settings['directory']}"
        with cls._lock:
            if key not in cls._instances:
                cls._instances[key] = cls(settings)
            return cls._instances[key]
    
    def __init__(self, settings):
        self.settings = settings
        self.ftp = None
        self.connected = False
        self.lock = threading.Lock()
        self.last_activity = time.time()
        self.connection_limit = FTP_CONFIG['MAX_CONNECTIONS']
        self.connection_semaphore = threading.BoundedSemaphore(self.connection_limit)
        self.error_count = 0
        self.last_error = None
        self.keepalive_timer = None
        self.idle_timer = None
        
    def _start_keepalive(self):
        """Rozpoczyna timer keepalive"""
        if self.keepalive_timer:
            self.keepalive_timer.cancel()
        self.keepalive_timer = threading.Timer(FTP_CONFIG['KEEPALIVE_INTERVAL'], self._send_keepalive)
        self.keepalive_timer.daemon = True
        self.keepalive_timer.start()
        
    def _send_keepalive(self):
        """Wysy≈Ça komendƒô NOOP do serwera FTP aby utrzymaƒá po≈ÇƒÖczenie"""
        try:
            with self.lock:
                if self.connected and self.ftp:
                    self.ftp.voidcmd('NOOP')
                    self.last_activity = time.time()
                    self._start_keepalive()
        except Exception as e:
            print(f"B≈ÇƒÖd keepalive: {str(e)}")
            self.connected = False
            
    def _start_idle_timer(self):
        """Rozpoczyna timer bezczynno≈õci"""
        if self.idle_timer:
            self.idle_timer.cancel()
        self.idle_timer = threading.Timer(FTP_CONFIG['IDLE_TIMEOUT'], self._handle_idle_timeout)
        self.idle_timer.daemon = True
        self.idle_timer.start()
        
    def _handle_idle_timeout(self):
        """Obs≈Çuguje timeout bezczynno≈õci"""
        with self.lock:
            if time.time() - self.last_activity >= FTP_CONFIG['IDLE_TIMEOUT']:
                self.close()
                
    def verify_connection(self):
        """Weryfikuje po≈ÇƒÖczenie FTP i zwraca szczeg√≥≈Çowy status"""
        try:
            if not self.connect():
                return False, "Nie mo≈ºna nawiƒÖzaƒá po≈ÇƒÖczenia FTP"
                
            # Testuj uprawnienia do zapisu
            with tempfile.NamedTemporaryFile() as tmp:
                tmp.write(b"test")
                tmp.seek(0)
                try:
                    test_filename = f"test_{uuid.uuid4().hex[:8]}.tmp"
                    self.ftp.storbinary(f'STOR {test_filename}', tmp)
                    self.ftp.delete(test_filename)
                except Exception as e:
                    return False, f"Brak uprawnie≈Ñ do zapisu: {str(e)}"
                
            return True, "Po≈ÇƒÖczenie FTP dzia≈Ça prawid≈Çowo"
        except Exception as e:
            return False, f"B≈ÇƒÖd weryfikacji FTP: {str(e)}"
    
    def connect(self):
        with self.lock:
            if self.connected and time.time() - self.last_activity < FTP_CONFIG['CONNECTION_TIMEOUT']:
                return True
                
            try:
                if self.ftp:
                    try:
                        self.ftp.quit()
                    except Exception as e:
                        print(f"B≈ÇƒÖd podczas zamykania poprzedniego po≈ÇƒÖczenia: {str(e)}")
                
                # Sprawd≈∫ czy u≈ºywamy FTPS
                use_tls = self.settings.get("use_tls", False)
                if use_tls:
                    self.ftp = ftplib.FTP_TLS()
                else:
                    self.ftp = ftplib.FTP()
                
                # Ustaw timeout po≈ÇƒÖczenia
                self.ftp.connect(
                    self.settings["host"], 
                    self.settings["port"], 
                    timeout=FTP_CONFIG['CONNECTION_TIMEOUT']
                )
                
                # Logowanie i konfiguracja TLS
                if use_tls:
                    self.ftp.auth()
                    self.ftp.prot_p()  # W≈ÇƒÖcz ochronƒô danych
                
                self.ftp.login(self.settings["username"], self.settings["password"])
                
                # Optymalizacja ustawie≈Ñ FTP
                self.ftp.set_pasv(True)  # Tryb pasywny czƒôsto jest bardziej niezawodny
                
                if self.settings["directory"] and self.settings["directory"] != "/":
                    try:
                        self.ftp.cwd(self.settings["directory"])
                    except ftplib.error_perm as e:
                        dirs = [d for d in self.settings["directory"].strip("/").split("/") if d]
                        for directory in dirs:
                            try:
                                self.ftp.cwd(directory)
                            except ftplib.error_perm:
                                self.ftp.mkd(directory)
                                self.ftp.cwd(directory)
                
                self.connected = True
                self.last_activity = time.time()
                self.error_count = 0
                
                # Uruchom timery
                self._start_keepalive()
                self._start_idle_timer()
                
                return True
            except Exception as e:
                self.last_error = str(e)
                self.error_count += 1
                self.connected = False
                raise
    
    def upload_file(self, file_path, remote_filename=None, max_retries=3):
        with self.connection_semaphore:
            if not remote_filename:
                remote_filename = os.path.basename(file_path)
                
            # Zmniejszamy op√≥≈∫nienie miƒôdzy pr√≥bami
            time.sleep(random.uniform(0.05, 0.2))
            
            if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
                return {"success": False, "error": f"Plik nie istnieje lub jest pusty: {file_path}"}
            
            for attempt in range(max_retries):
                with self.lock:
                    if not self.connected and not self.connect():
                        if attempt == max_retries - 1:
                            return {"success": False, "error": f"Nie mo≈ºna po≈ÇƒÖczyƒá siƒô z FTP: {self.last_error}"}
                        time.sleep(min(FTP_CONFIG['MAX_RETRY_DELAY'], 
                                    FTP_CONFIG['DEFAULT_RETRY_DELAY'] * (attempt + 1)))
                        continue
                
                try:
                    with open(file_path, 'rb') as file:
                        with self.lock:
                            self.ftp.storbinary(f'STOR {remote_filename}', file)
                            self.last_activity = time.time()
                            self._start_idle_timer()  # Resetuj timer bezczynno≈õci

                    # Buduj URL
                    if self.settings.get("http_path"):
                        http_path = self.settings["http_path"].strip()
                        if not http_path.endswith('/'): http_path += '/'
                        image_url = f"{http_path}{remote_filename}"
                    else:
                        scheme = "ftps" if self.settings.get("use_tls") else "ftp"
                        image_url = f"{scheme}://{self.settings['host']}"
                        if self.settings["directory"] and self.settings["directory"] != "/":
                            if not self.settings["directory"].startswith("/"): image_url += "/"
                            image_url += self.settings["directory"]
                            if not image_url.endswith("/"): image_url += "/"
                        else:
                            image_url += "/"
                        image_url += remote_filename

                    return {"success": True, "url": image_url, "filename": remote_filename}
                except Exception as e:
                    print(f"B≈ÇƒÖd wysy≈Çania FTP (pr√≥ba {attempt+1}): {str(e)}")
                    self.last_error = str(e)
                    self.error_count += 1
                    self.connected = False
                    
                    if attempt < max_retries - 1:
                        time.sleep(min(FTP_CONFIG['MAX_RETRY_DELAY'], 
                                    FTP_CONFIG['DEFAULT_RETRY_DELAY'] * (attempt + 1)))
                    
            return {"success": False, "error": f"Nie uda≈Ço siƒô wys≈Çaƒá pliku po {max_retries} pr√≥bach. Ostatni b≈ÇƒÖd: {self.last_error}"}
    
    def close(self):
        with self.lock:
            if self.keepalive_timer:
                self.keepalive_timer.cancel()
            if self.idle_timer:
                self.idle_timer.cancel()
            if self.connected and self.ftp:
                try: self.ftp.quit()
                except: pass
                self.connected = False
                
    def __del__(self):
        self.close()

class FTPConnectionPool:
    def __init__(self, max_connections=5):
        self.pool = ThreadPoolExecutor(max_workers=max_connections)
        self.connections = []
        self.lock = threading.Lock()
        
    def get_connection(self):
        with self.lock:
            if not self.connections:
                return FTPManager.get_instance(st.session_state.ftp_settings)
            return self.connections.pop()
            
    def release_connection(self, connection):
        with self.lock:
            self.connections.append(connection)
            
    def close_all(self):
        with self.lock:
            for conn in self.connections:
                try:
                    conn.close()
                except:
                    pass
            self.connections.clear()

def calculate_batch_size(total_urls, max_workers):
    return min(
        max(FTP_CONFIG['MIN_BATCH_SIZE'], total_urls // 10),
        max_workers * 2
    )

class FTPBatchManager:
    """
    Manager do wykonywania operacji FTP w batchach,
    aby zmniejszyƒá obciƒÖ≈ºenie serwera FTP i uniknƒÖƒá blokowania po≈ÇƒÖcze≈Ñ.
    """
    
    def __init__(self, settings, max_workers):
        self.settings = settings
        self.max_workers = max_workers
        self.batch_size = calculate_batch_size(max_workers * 10, max_workers)
        self.connection_pool = FTPConnectionPool(max_workers)
        self.upload_queue = queue.Queue()
        self.results = {}
        self.running = False
        self.worker_thread = None
        self.lock = threading.Lock()
        self.semaphore = threading.Semaphore(max_workers)
        self.ftp_manager = FTPManager.get_instance(settings)
        self.stats = {
            'total_uploaded': 0,
            'failed_uploads': 0,
            'retry_count': 0,
            'start_time': None,
            'last_upload_time': None
        }
        
    def verify_connection(self):
        """Weryfikuje po≈ÇƒÖczenie FTP przed rozpoczƒôciem przetwarzania"""
        return self.ftp_manager.verify_connection()
        
    def add_upload_task(self, file_path, callback=None):
        """Dodaje zadanie do kolejki przesy≈Çania"""
        task_id = str(uuid.uuid4())
        self.upload_queue.put((task_id, file_path, callback))
        self.results[task_id] = {"status": "queued", "file_path": file_path}
        return task_id
        
    def start_processing(self):
        """Rozpoczyna przetwarzanie zada≈Ñ w osobnym wƒÖtku"""
        if self.running:
            return
            
        self.running = True
        self.stats['start_time'] = time.time()
        self.worker_thread = threading.Thread(target=self._process_queue)
        self.worker_thread.daemon = True
        self.worker_thread.start()
        
    def stop_processing(self):
        """Zatrzymuje przetwarzanie zada≈Ñ"""
        self.running = False
        if self.worker_thread:
            if self.worker_thread.is_alive():
                self.worker_thread.join(timeout=3)
            self.worker_thread = None
            
    def _process_queue(self):
        """Przetwarza zadania wysy≈Çania w kolejce"""
        while self.running:
            try:
                batch = []
                for _ in range(self.batch_size):
                    if self.upload_queue.empty():
                        break
                    batch.append(self.upload_queue.get(block=False))
                
                if not batch:
                    time.sleep(0.1)  # Zmniejszone op√≥≈∫nienie
                    continue
                
                time.sleep(random.uniform(0.1, 0.3))  # Zmniejszone op√≥≈∫nienie miƒôdzy batchami
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = {executor.submit(self._upload_file, task_id, file_path): (task_id, callback) 
                              for task_id, file_path, callback in batch}
                    
                    for future in concurrent.futures.as_completed(futures):
                        task_id, callback = futures[future]
                        try:
                            result = future.result()
                            self.results[task_id] = result
                            
                            # Aktualizuj statystyki
                            if result.get("status") == "success":
                                self.stats['total_uploaded'] += 1
                            elif result.get("status") == "error":
                                self.stats['failed_uploads'] += 1
                            self.stats['last_upload_time'] = time.time()
                            
                            if callback:
                                callback(task_id, result)
                        except Exception as e:
                            error_result = {
                                "status": "error", 
                                "error": f"B≈ÇƒÖd wysy≈Çania: {str(e)}",
                                "details": getattr(e, 'details', None)
                            }
                            self.results[task_id] = error_result
                            self.stats['failed_uploads'] += 1
                            if callback:
                                callback(task_id, error_result)
                
            except queue.Empty:
                time.sleep(0.1)
            except Exception as e:
                print(f"FTPBatchManager error: {str(e)}")
                time.sleep(0.5)
                
    def _upload_file(self, task_id, file_path):
        """Przesy≈Ça pojedynczy plik na FTP"""
        with self.semaphore:
            try:
                self.results[task_id] = {"status": "uploading"}
                upload_result = self.ftp_manager.upload_file(file_path)
                
                if upload_result["success"]:
                    return {
                        "status": "success", 
                        "url": upload_result["url"], 
                        "filename": upload_result["filename"]
                    }
                else:
                    self.stats['retry_count'] += 1
                    return {"status": "error", "error": upload_result["error"]}
            except Exception as e:
                self.stats['retry_count'] += 1
                return {"status": "error", "error": str(e)}
                
    def get_result(self, task_id):
        """Pobiera wynik dla zadania"""
        with self.lock:
            return self.results.get(task_id)
            
    def get_stats(self):
        """Zwraca statystyki przetwarzania"""
        with self.lock:
            stats = self.stats.copy()
            if stats['start_time']:
                stats['elapsed_time'] = time.time() - stats['start_time']
                if stats['total_uploaded'] > 0 and stats['elapsed_time'] > 0:
                    stats['upload_rate'] = stats['total_uploaded'] / stats['elapsed_time']
                else:
                    stats['upload_rate'] = 0
            return stats

# Funkcje pomocnicze
def authenticate_user():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if not st.session_state.authenticated:
        st.title("Pobieranie zdjƒôƒá z XML/CSV - Logowanie")
        user = st.text_input("Login")
        password = st.text_input("Has≈Ço", type="password")
        if st.button("Zaloguj"):
            if user == st.secrets.get("APP_USER") and password == st.secrets.get("APP_PASSWORD"):
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("Nieprawid≈Çowy login lub has≈Ço")
        st.stop()
    return True

def initialize_session_state():
    defaults = {
        "generated_code": "", "edited_code": "", "output_bytes": None,
        "file_info": None, "show_editor": False, "error_info": None,
        "code_fixed": False, "fix_requested": False, "downloaded_images": [],
        "ftp_settings": {
            "host": "", "port": 21, "username": "", "password": "",
            "directory": "/", "http_path": ""
        },
        "processing_params": {
            "xpath": "", "column_name": "", "new_node_name": "",
            "new_column_name": "", "separator": ","
        }
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

def detect_encoding(raw_bytes):
    """Wykrywa kodowanie pliku"""
    # Sprawd≈∫ BOM
    if raw_bytes.startswith(codecs.BOM_UTF16_LE):
        return 'utf-16-le'
    if raw_bytes.startswith(codecs.BOM_UTF16_BE):
        return 'utf-16-be'
    if raw_bytes.startswith(codecs.BOM_UTF8):
        return 'utf-8'
    
    # Sprawd≈∫ deklaracjƒô XML
    encoding_match = re.search(br'<\?xml[^>]*encoding=["\']([^"\']+)["\']', raw_bytes)
    if encoding_match:
        try:
            return encoding_match.group(1).decode('ascii').lower()
        except:
            pass
    
    # Pr√≥buj popularne kodowania
    for enc in ["utf-8", "iso-8859-2", "windows-1250"]:
        try:
            raw_bytes.decode(enc)
            return enc
        except:
            continue
    
    return 'utf-8'  # Domy≈õlne kodowanie

def read_file_content(uploaded_file):
    if not uploaded_file:
        return None, "Nie wybrano pliku"
    
    try:
        raw_bytes = uploaded_file.read()
        file_type = uploaded_file.name.split(".")[-1].lower()
        
        if file_type not in ["xml", "csv"]:
            return None, "Nieobs≈Çugiwany typ pliku. Akceptowane formaty to XML i CSV."
        
        encoding = detect_encoding(raw_bytes)
        content = raw_bytes.decode(encoding)
        
        if file_type == "csv":
            # Weryfikacja czy to poprawny CSV
            try:
                pd.read_csv(io.StringIO(content))
            except:
                return None, "Niepoprawny format CSV"
        
        return {
            "content": content,
            "raw_bytes": raw_bytes,
            "type": file_type,
            "encoding": encoding,
            "name": uploaded_file.name
        }, None
        
    except Exception as e:
        return None, f"B≈ÇƒÖd podczas odczytu pliku: {str(e)}"

def download_image(url, temp_dir, log_list=None):
    try:
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            if log_list is not None:
                log_list.append(f"ERROR: Nieprawid≈Çowy URL: {url}")
            return None, f"Nieprawid≈Çowy URL: {url}"

        headers = {
            "User-Agent": "Mozilla/5.0", "Accept": "*/*",
            "Referer": f"{parsed_url.scheme}://{parsed_url.netloc}/"
        }

        # Specjalna obs≈Çuga dla image_show.php
        if "image_show.php" in url:
            html_resp = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            html_resp.raise_for_status()
            soup = BeautifulSoup(html_resp.text, "html.parser")
            img_tag = soup.find("img")
            if not img_tag or not img_tag.get("src"):
                if log_list is not None:
                    log_list.append(f"WARNING: Brak <img> w HTML dla {url}")
                return None, "Nie znaleziono znacznika <img> w odpowiedzi HTML"
            img_src = img_tag["src"]
            img_url = f"{parsed_url.scheme}://{parsed_url.netloc}/{img_src.lstrip('/')}" if not img_src.startswith("http") else img_src
        else:
            img_url = url

        for retry in range(3):
            try:
                response = requests.get(img_url, headers=headers, stream=False, timeout=15, allow_redirects=True)
                if log_list is not None:
                    log_list.append(f"INFO: {img_url} -> HTTP {response.status_code}, Content-Type: {response.headers.get('Content-Type')}")
                response.raise_for_status()
                extension = {
                    "image/jpeg": ".jpg", "image/png": ".png",
                    "image/gif": ".gif", "image/webp": ".webp"
                }.get(response.headers.get("Content-Type", ""), ".jpg")
                filename = f"image_{uuid.uuid4().hex}{extension}"
                file_path = os.path.join(temp_dir, filename)
                with open(file_path, "wb") as f:
                    f.write(response.content)
                file_size = os.path.getsize(file_path)
                if log_list is not None:
                    log_list.append(f"INFO: Zapisano {file_path}, rozmiar: {file_size} bajt√≥w")
                if file_size > 100 and file_size < 20 * 1024 * 1024:
                    return {"path": file_path, "filename": filename, "original_url": url}, None
                else:
                    os.remove(file_path)
                    if retry < 2:
                        continue
                    if file_size <= 100:
                        if log_list is not None:
                            log_list.append(f"WARNING: Plik za ma≈Çy: {file_size} bajt√≥w dla {url}")
                        return None, f"Pobrany plik jest zbyt ma≈Çy (rozmiar: {file_size})"
                    else:
                        if log_list is not None:
                            log_list.append(f"WARNING: Plik za du≈ºy: {file_size} bajt√≥w dla {url}")
                        return None, f"Plik jest zbyt du≈ºy (>20MB, rozmiar: {file_size})"
            except Exception as e:
                if log_list is not None:
                    log_list.append(f"ERROR: {img_url} -> {str(e)}")
                if retry == 2:
                    return None, f"B≈ÇƒÖd przy pobieraniu: {str(e)}"
    except Exception as e:
        if log_list is not None:
            log_list.append(f"ERROR (outer): {url} -> {str(e)}")
        return None, f"B≈ÇƒÖd: {str(e)}"

def process_single_url(url, retry_count=0, temp_dir=None, max_retries=3, log_list=None):
    """Pomocnicza funkcja do przetwarzania pojedynczego URL"""
    try:
        if log_list is not None:
            log_list.append(f"‚¨áÔ∏è Pobieranie: {url}")
        image_info, error = download_image(url, temp_dir, log_list=log_list)
        
        if error:
            # Dodane bardziej szczeg√≥≈Çowe komunikaty dla czƒôstych b≈Çƒôd√≥w
            if "image_show.php" in url and "Nie znaleziono znacznika" in error:
                specific_error = f"Problem z parsowaniem HTML: {error}"
            elif "timeout" in error.lower():
                specific_error = f"Timeout podczas pobierania: {error}"
            elif "connection" in error.lower():
                specific_error = f"Problem z po≈ÇƒÖczeniem: {error}"
            else:
                specific_error = error
                
            if retry_count < max_retries:
                # Zwiƒôksz czas oczekiwania miƒôdzy kolejnymi pr√≥bami
                wait_time = 1 + retry_count * 2  # 1s, 3s, 5s...
                if log_list is not None:
                    log_list.append(f"üîÑ Ponawiam {retry_count + 1}/{max_retries} dla {url}: {specific_error} (czekam {wait_time}s)")
                time.sleep(wait_time)
                return {
                    "status": "retry",
                    "url": url,
                    "retry_count": retry_count + 1,
                    "error": specific_error
                }
            else:
                if log_list is not None:
                    log_list.append(f"‚ùå B≈ÇƒÖd pobierania dla {url}: {specific_error}")
                return {
                    "status": "error",
                    "url": url,
                    "error": specific_error
                }
        
        if not image_info or not image_info.get("path"):
            return {
                "status": "error",
                "url": url,
                "error": "Brak informacji o pobranym pliku"
            }
            
        if log_list is not None:
            log_list.append(f"‚úÖ Pobrano: {url}")
            
        return {
            "status": "success",
            "url": url,
            "image_info": image_info
        }
        
    except Exception as e:
        # Poprawiona obs≈Çuga wyjƒÖtk√≥w
        exception_details = f"{type(e).__name__}: {str(e)}"
        if retry_count < max_retries:
            wait_time = 1 + retry_count * 2
            if log_list is not None:
                log_list.append(f"üîÑ Ponawiam {retry_count + 1}/{max_retries} dla {url}: {exception_details} (czekam {wait_time}s)")
            time.sleep(wait_time)
            return {
                "status": "retry",
                "url": url,
                "retry_count": retry_count + 1,
                "error": exception_details
            }
        else:
            if log_list is not None:
                log_list.append(f"‚õî B≈ÇƒÖd dla {url}: {exception_details}")
            return {
                "status": "error",
                "url": url,
                "error": exception_details
            }

def process_images_in_parallel(urls, temp_dir, ftp_settings, max_workers=None, debug_container=None, max_retries=3, progress_callback=None, session_id=None):
    # U≈ºyj domy≈õlnej warto≈õci je≈õli nie podano max_workers
    if max_workers is None:
        max_workers = FTP_CONFIG['DEFAULT_WORKERS']
    
    # Upewnij siƒô, ≈ºe max_workers jest w dozwolonym zakresie
    max_workers = max(FTP_CONFIG['MIN_WORKERS'], 
                     min(FTP_CONFIG['MAX_WORKERS'], max_workers))
    
    # Inicjalizacja FTP Batch Managera z odpowiednimi parametrami
    ftp_batch_manager = FTPBatchManager(ftp_settings, max_workers)
    
    if debug_container:
        debug_container.info(f"Konfiguracja przetwarzania:")
        debug_container.info(f"- Max r√≥wnoleg≈Çych proces√≥w: {max_workers}")
        debug_container.info(f"- Rozmiar batcha: {ftp_batch_manager.batch_size}")
        debug_container.info(f"- Max po≈ÇƒÖcze≈Ñ FTP: {ftp_batch_manager.max_workers}")
    
    # Weryfikacja po≈ÇƒÖczenia FTP przed rozpoczƒôciem
    success, message = ftp_batch_manager.verify_connection()
    if not success:
        if debug_container:
            debug_container.error(f"‚ùå B≈ÇƒÖd po≈ÇƒÖczenia FTP: {message}")
        return {}, [], [{"url": "all", "error": f"B≈ÇƒÖd po≈ÇƒÖczenia FTP: {message}"}]
    
    if debug_container:
        debug_container.success("‚úÖ Po≈ÇƒÖczenie FTP zweryfikowane pomy≈õlnie")
    
    new_urls_map = {}
    downloaded_images = []
    failed_urls = []
    retry_queue = queue.Queue()
    
    # Initialize counters
    total_urls = len(urls)
    processed_count = 0
    
    # Inicjalizacja postƒôpu - poka≈º 0% na starcie
    if progress_callback:
        progress_callback(0, 0, 0, total_urls)
        
    if debug_container:
        debug_container.info(f"Inicjalizacja przetwarzania dla {total_urls} obraz√≥w...")
    
    # Inicjalizuj FTP Batch Manager
    ftp_batch_manager.start_processing()
    
    # S≈Çownik do ≈õledzenia zada≈Ñ FTP
    ftp_tasks = {}
    ftp_results_lock = threading.Lock()
    
    def check_ftp_tasks():
        """Sprawdza status zada≈Ñ FTP i aktualizuje wyniki"""
        nonlocal processed_count, new_urls_map
        completed_tasks = []
        
        with ftp_results_lock:
            for url, task_info in ftp_tasks.items():
                result = ftp_batch_manager.get_result(task_info["task_id"])
                if not result:
                    # Zadanie jeszcze nie zako≈Ñczone
                    continue
                    
                if result["status"] == "success":
                    new_urls_map[url] = result["url"]
                    downloaded_images.append({
                        "original_url": url,
                        "ftp_url": result["url"],
                        "filename": result["filename"]
                    })
                    completed_tasks.append(url)
                    processed_count += 1
                    if debug_container:
                        debug_container.success(f"‚úÖ FTP zako≈Ñczone: {url}")
                elif result["status"] == "error":
                    # Bardziej szczeg√≥≈Çowa diagnostyka b≈Çƒôd√≥w FTP
                    error_details = result.get("error", "Nieznany b≈ÇƒÖd FTP")
                    if "connection" in error_details.lower():
                        error_category = "B≈ÇƒÖd po≈ÇƒÖczenia FTP"
                    elif "permission" in error_details.lower() or "access" in error_details.lower():
                        error_category = "B≈ÇƒÖd uprawnie≈Ñ FTP"
                    elif "disk" in error_details.lower() or "space" in error_details.lower():
                        error_category = "B≈ÇƒÖd przestrzeni dyskowej"
                    else:
                        error_category = "B≈ÇƒÖd FTP"
                        
                    if task_info["retry_count"] < max_retries:
                        new_retry_count = task_info["retry_count"] + 1
                        # Zwiƒôksz czas oczekiwania miƒôdzy pr√≥bami
                        wait_time = min(5, 1 * (new_retry_count))
                        time.sleep(wait_time)
                        
                        retry_queue.put((url, new_retry_count))
                        completed_tasks.append(url)
                        
                        if debug_container:
                            debug_container.warning(f"üîÑ Ponawiam FTP {new_retry_count}/{max_retries} dla {url}: {error_details}")
                    else:
                        failed_urls.append({"url": url, "error": f"{error_category}: {error_details}"})
                        completed_tasks.append(url)
                        processed_count += 1
                        if debug_container:
                            debug_container.error(f"‚ùå FTP nieudane: {url}: {error_details}")
            
            # Usu≈Ñ zako≈Ñczone zadania
            for url in completed_tasks:
                if url in ftp_tasks:
                    del ftp_tasks[url]
    
    # Dodajemy tylko poczƒÖtkowy batch URL-i
    initial_batch_size = min(max_workers * 2, total_urls)
    for url in urls[:initial_batch_size]:
        retry_queue.put((url, 0))
    
    next_url_index = initial_batch_size
    
    # Process until queue is empty and all URLs are processed
    log_list = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        while not retry_queue.empty() or next_url_index < total_urls or ftp_tasks:
            # Sprawd≈∫ status zada≈Ñ FTP
            check_ftp_tasks()
            
            # Dodajemy wiƒôcej URL-i do kolejki w miarƒô potrzeby
            while next_url_index < total_urls and retry_queue.qsize() < max_workers * 2:
                retry_queue.put((urls[next_url_index], 0))
                next_url_index += 1
                
            # Informacja o stanie kolejki
            queue_size = retry_queue.qsize()
            if debug_container and (next_url_index < total_urls or queue_size > 0 or ftp_tasks):
                ftp_queue_size = len(ftp_tasks)
                debug_container.info(f"üìã W kolejce pobierania: {queue_size} | W kolejce FTP: {ftp_queue_size} | Zaplanowano: {next_url_index}/{total_urls}")
            
            if retry_queue.empty() and next_url_index >= total_urls and ftp_tasks:
                time.sleep(0.1)
                continue
            
            # Get batch of URLs from queue
            batch = []
            batch_size = min(max_workers, queue_size)
            for _ in range(batch_size):
                if not retry_queue.empty():
                    batch.append(retry_queue.get())
            
            if not batch:
                if next_url_index < total_urls:
                    continue
                elif ftp_tasks:
                    time.sleep(0.1)
                    continue
                else:
                    break
            
            # Process batch
            futures = {
                executor.submit(
                    process_single_url,
                    url=url,
                    retry_count=retry_count,
                    temp_dir=temp_dir,
                    max_retries=max_retries,
                    log_list=log_list
                ): (url, retry_count) for url, retry_count in batch
            }
            
            # --- Zmienna do liczenia ile zdjƒôƒá od ostatniej aktualizacji ---
            images_since_update = 0
            batch_size_total = len(futures)
            batch_processed = 0
            # ---
            for future in concurrent.futures.as_completed(futures):
                url, retry_count = futures[future]
                try:
                    result = future.result()
                    
                    if result["status"] == "success":
                        # Dodaj zadanie do kolejki FTP
                        task_id = ftp_batch_manager.add_upload_task(result["image_info"]["path"])
                        with ftp_results_lock:
                            ftp_tasks[url] = {
                                "task_id": task_id,
                                "file_path": result["image_info"]["path"],
                                "retry_count": retry_count
                            }
                        if debug_container:
                            debug_container.info(f"‚¨ÜÔ∏è Zadanie FTP dodane: {url}")
                    elif result["status"] == "retry":
                        if debug_container:
                            debug_container.info(f"üîÑ Ponawiam {result['retry_count']}/{max_retries} dla {url}: {result.get('error')}")
                        time.sleep(random.uniform(0.5, 1.0))
                        retry_queue.put((url, result["retry_count"]))
                    else:  # error
                        failed_urls.append({"url": url, "error": result.get("error", "Nieznany b≈ÇƒÖd")})
                        processed_count += 1
                        if debug_container:
                            debug_container.warning(f"‚ùå B≈ÇƒÖd dla {url}: {result.get('error')}")

                    # --- Aktualizacja postƒôpu co 10 zdjƒôƒá lub na ko≈Ñcu batcha ---
                    batch_processed += 1
                    images_since_update += 1
                    if progress_callback and total_urls > 0:
                        if images_since_update >= 10 or batch_processed == batch_size_total:
                            remaining = total_urls - next_url_index + retry_queue.qsize() + len(ftp_tasks)
                            progress = min(0.99, max(0.01, (total_urls - remaining) / total_urls))
                            stats = ftp_batch_manager.get_stats()
                            progress_callback(progress, processed_count, stats['total_uploaded'], total_urls)
                            images_since_update = 0
                            save_current_state()  # Zapisz stan co 10 obraz√≥w
                    # ---
                except Exception as e:
                    failed_urls.append({"url": url, "error": str(e)})
                    processed_count += 1
                    if debug_container:
                        debug_container.error(f"‚õî WyjƒÖtek dla {url}: {str(e)}")
    
    # Zatrzymaj FTP Batch Manager
    ftp_batch_manager.stop_processing()
    
    # Final progress update
    if progress_callback and total_urls > 0:
        progress_callback(1.0, processed_count, processed_count, total_urls)
    
    # Close all FTP connections
    for instance in FTPManager._instances.values():
        instance.close()
    
    # Po zako≈Ñczeniu batcha wy≈õwietl logi
    if log_list:
        with st.expander("Logi debugowania (pobieranie obraz√≥w)"):
            for log in log_list:
                st.write(log)
    
    return new_urls_map, downloaded_images, failed_urls

def handle_errors(func):
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            return None, f"B≈ÇƒÖd: {str(e)}"
    return wrapper

@handle_errors
def extract_image_urls_from_xml(xml_content, xpath_expression, separator=","):
    if not xml_content or not xml_content.strip():
        return None, "Plik XML jest pusty"
    
    # Oczy≈õƒá dane wej≈õciowe
    if xml_content.startswith("\ufeff"):
        xml_content = xml_content[1:]
    xml_content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', xml_content)
    
    # Obs≈Çuga atrybut√≥w
    is_attribute = '/@' in xpath_expression
    attribute_name = xpath_expression.split('/@')[-1] if is_attribute else None
    xpath_base = xpath_expression.split('/@')[0] if is_attribute else xpath_expression

    # Wyra≈ºenia regularne dla najczƒôstszych przypadk√≥w
    if xpath_base in ["//product/image", "product/image", "//image", "/image"]:
        try:
            # Ulepszone wyra≈ºenia regularne dla r√≥≈ºnych format√≥w
            pattern_simple = re.compile(r'<image>(.*?)</image>', re.DOTALL)
            pattern_cdata = re.compile(r'<image><!\[CDATA\[(.*?)\]\]></image>', re.DOTALL)
            pattern_with_attribs = re.compile(r'<image [^>]*?>(.*?)</image>', re.DOTALL)
            
            matches = pattern_simple.findall(xml_content) + pattern_cdata.findall(xml_content) + pattern_with_attribs.findall(xml_content)
            
            urls = []
            for match in matches:
                match = match.strip()
                if not match:
                    continue
                
                # Znajd≈∫ URL-e w tre≈õci
                url_pattern = re.compile(r'https?://[^\s<>"\']+')
                found_urls = url_pattern.findall(match)
                
                if found_urls:
                    for url in found_urls:
                        clean_url = url.replace('&amp;', '&')
                        urls.append(clean_url)
                elif 'http://' in match or 'https://' in match:
                    urls.append(match.replace('&amp;', '&'))
            
            if urls:
                return urls, None
        except Exception as e:
            return None, f"B≈ÇƒÖd przy parsowaniu XML wyra≈ºeniami regularnymi: {str(e)}"

    # ElementTree dla innych przypadk√≥w
    try:
        # Pr√≥buj naprawiƒá czƒôste problemy w XML
        xml_content = re.sub(r'&(?!amp;|lt;|gt;|apos;|quot;)', '&amp;', xml_content)
        
        root = ET.fromstring(xml_content)
        xpath = f"./{xpath_base[2:]}" if xpath_base.startswith('//') else f"./{xpath_base}" if not xpath_base.startswith('./') else xpath_base
        elements = root.findall(xpath)

        urls = []
        for element in elements:
            element_text = element.attrib.get(attribute_name) if is_attribute else element.text
            if element_text:
                element_text = element_text.replace('&amp;', '&')
                
                # Sprawd≈∫, czy tekst zawiera URL-e
                if 'http://' in element_text or 'https://' in element_text:
                    if separator in element_text:
                        found_urls = [url.strip() for url in element_text.split(separator) 
                                    if url.strip() and ('http://' in url or 'https://' in url)]
                        urls.extend(found_urls)
                    else:
                        urls.append(element_text.strip())
        
        # Je≈õli nie znaleziono URL-i, spr√≥buj bardziej generyczne podej≈õcie
        if not urls:
            # Znajd≈∫ wszystkie elementy z tekstem zawierajƒÖcym http
            for elem in root.iter():
                if elem.text and ('http://' in elem.text or 'https://' in elem.text):
                    element_text = elem.text.replace('&amp;', '&')
                    if separator in element_text:
                        found_urls = [url.strip() for url in element_text.split(separator) 
                                    if url.strip() and ('http://' in url or 'https://' in url)]
                        urls.extend(found_urls)
                    else:
                        urls.append(element_text.strip())
        
        return urls, None
    except ET.ParseError as e:
        return None, f"B≈ÇƒÖd przy parsowaniu XML: {str(e)}"

def update_xml_with_new_urls(xml_content, xpath_expression, new_urls_map, new_node_name, separator=","):
    try:
        if not xml_content.strip() or not new_node_name.strip():
            return None, "Plik XML jest pusty lub nazwa wƒôz≈Ça jest pusta"

        # Obs≈Çuga atrybut√≥w
        is_attribute = '/@' in xpath_expression
        attribute_name = xpath_expression.split('/@')[-1] if is_attribute else None
        xpath_base = xpath_expression.split('/@')[0] if is_attribute else xpath_expression

        root = ET.fromstring(xml_content)
        xpath = xpath_base[2:] if xpath_base.startswith('//') else xpath_base
        elements = root.findall(f'.//{xpath}')
        
        # Mapa rodzic-dziecko dla szybkiego odnajdywania
        parent_map = {c: p for p in root.iter() for c in p}
        
        # Mapa rodzic-elementy do przetworzenia
        parent_to_elements = {}
        for element in elements:
            parent = parent_map.get(element)
            if parent is None:
                continue
            if parent not in parent_to_elements:
                parent_to_elements[parent] = []
            parent_to_elements[parent].append(element)
        
        # Przetwarzanie element√≥w
        for parent, elements_list in parent_to_elements.items():
            # Znajd≈∫ lub utw√≥rz wƒôze≈Ç ftp_images
            ftp_images = None
            for child in parent:
                if child.tag == "ftp_images":
                    ftp_images = child
                    break
            if ftp_images is None:
                ftp_images = ET.Element("ftp_images")
                parent.append(ftp_images)
            
            # Przetwarzanie element√≥w
            for element in elements_list:
                # Pobierz oryginalny URL
                original_url = element.attrib.get(attribute_name, "").strip() if is_attribute else (element.text.strip() if element.text else "")
                if not original_url:
                    continue
                
                # Obs≈Çuga wielu URL-i w jednym elemencie
                if separator in original_url:
                    urls = [url.strip() for url in original_url.split(separator)]
                    new_urls = [new_urls_map[url] for url in urls if url in new_urls_map]
                    if new_urls:
                        ftp_node = ET.Element(new_node_name)
                        ftp_node.text = separator.join(new_urls)
                        ftp_images.append(ftp_node)
                # Obs≈Çuga pojedynczego URL-a
                elif original_url in new_urls_map:
                    ftp_node = ET.Element(new_node_name)
                    ftp_node.text = new_urls_map[original_url]
                    ftp_images.append(ftp_node)

        return ET.tostring(root, encoding="unicode"), None
    except Exception as e:
        return None, f"B≈ÇƒÖd przy aktualizacji XML: {str(e)}"

@handle_errors
def extract_image_urls_from_csv(csv_content, column_name, separator=","):
    df = pd.read_csv(io.StringIO(csv_content))
    if column_name not in df.columns:
        return None, f"Kolumna '{column_name}' nie istnieje w pliku CSV."
    
    urls = []
    for value in df[column_name]:
        if pd.notna(value):
            if separator in str(value):
                urls.extend([url.strip() for url in str(value).split(separator) if url.strip()])
            else:
                urls.append(str(value).strip())
    
    return urls, None

def update_csv_with_new_urls(csv_content, column_name, new_urls_map, new_column_name, separator=","):
    try:
        if not new_column_name.strip():
            return None, "Nazwa nowej kolumny nie mo≈ºe byƒá pusta"

        df = pd.read_csv(io.StringIO(csv_content))
        if column_name not in df.columns:
            return None, f"Kolumna '{column_name}' nie istnieje w pliku CSV."

        if new_column_name not in df.columns:
            df[new_column_name] = ""

        for idx, value in enumerate(df[column_name]):
            if pd.notna(value):
                value_str = str(value).strip()
                
                # Obs≈Çuga wielu URL-i
                if separator in value_str:
                    urls = [url.strip() for url in value_str.split(separator)]
                    new_urls = [new_urls_map[url] for url in urls if url in new_urls_map]
                    if new_urls:
                        df.at[idx, new_column_name] = separator.join(new_urls)
                # Obs≈Çuga pojedynczego URL-a
                elif value_str in new_urls_map:
                    df.at[idx, new_column_name] = new_urls_map[value_str]
                else:
                    # Pr√≥ba dopasowania bez bia≈Çych znak√≥w
                    for key in new_urls_map:
                        if value_str.replace(" ", "") == key.replace(" ", ""):
                            df.at[idx, new_column_name] = new_urls_map[key]
                            break

        return df.to_csv(index=False), None
    except Exception as e:
        return None, f"B≈ÇƒÖd przy aktualizacji CSV: {str(e)}"

def save_to_google_drive(output_bytes, file_info, new_urls_map=None):
    try:
        drive_folder_id = st.secrets.get("GOOGLE_DRIVE_FOLDER_ID")
        credentials_json = st.secrets.get("GOOGLE_DRIVE_CREDENTIALS_JSON")
        
        if not drive_folder_id or not credentials_json:
            return False, "Brak konfiguracji Google Drive."

        now = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        log_filename = f"image_urls_map_{now}.txt"
        result_filename = f"processed_{now}.{file_info['type']}"
        
        with tempfile.TemporaryDirectory() as tmpdirname:
            # Zapisz pliki tymczasowo
            temp_result_path = os.path.join(tmpdirname, f"output.{file_info['type']}")
            temp_log_path = os.path.join(tmpdirname, "log.txt")
            
            with open(temp_result_path, "wb") as f:
                f.write(output_bytes)
                
            # Przygotuj log
            log_content = f"# Raport z przetwarzania obraz√≥w - {now}\n\n## Informacje o pliku\n"
            log_content += f"- Nazwa pliku: {file_info['name']}\n- Typ pliku: {file_info['type'].upper()}\n"
            log_content += f"- Kodowanie: {file_info['encoding']}\n\n## Mapowanie URL-i obraz√≥w\n\n"
            
            if new_urls_map:
                for i, (original_url, new_url) in enumerate(new_urls_map.items(), 1):
                    log_content += f"### Obraz #{i}\n- Oryginalny URL: {original_url}\n- Nowy URL: {new_url}\n\n"
            else:
                log_content += "Brak mapowania URL-i\n"
                
            with open(temp_log_path, "w", encoding='utf-8') as f:
                f.write(log_content)
            
            # Uwierzytelnianie Google Drive
            with st.spinner("Zapisujƒô na Google Drive..."):
                # Przygotuj po≈õwiadczenia
                if isinstance(credentials_json, str):
                    try:
                        creds_dict = json.loads(credentials_json)
                    except:
                        return False, "B≈ÇƒÖd dekodowania JSON z credentials"
                else:
                    creds_dict = credentials_json
                
                scope = ["https://www.googleapis.com/auth/drive"]
                credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
                
                gauth = GoogleAuth()
                gauth.credentials = credentials
                drive = GoogleDrive(gauth)
                
                try:
                    # Prze≈õlij pliki
                    log_file = drive.CreateFile({
                        "title": log_filename, 
                        "parents": [{"id": drive_folder_id}],
                        "mimeType": "text/plain"
                    })
                    log_file.SetContentFile(temp_log_path)
                    log_file.Upload()
                    
                    result_file = drive.CreateFile({
                        "title": result_filename, 
                        "parents": [{"id": drive_folder_id}],
                        "mimeType": f"application/{file_info['type']}"
                    })
                    result_file.SetContentFile(temp_result_path)
                    result_file.Upload()
                    
                    return True, "Pliki zosta≈Çy zapisane na Google Drive."
                except Exception as e:
                    return False, f"B≈ÇƒÖd podczas wysy≈Çania: {str(e)}"
    except Exception as e:
        return False, f"B≈ÇƒÖd Google Drive: {str(e)}"

# Funkcje zarzƒÖdzania stanem
def save_processing_state(session_id, urls, processed_urls, new_urls_map, file_info, processing_params):
    # Ensure all lists have unique entries
    all_urls = list(set(urls))
    processed_urls = list(set(processed_urls))
    
    # Calculate remaining URLs accurately
    remaining_urls = [url for url in all_urls if url not in processed_urls]
    
    state = {
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "file_info": {k: file_info[k] for k in ["name", "type", "encoding"]},
        "processing_params": processing_params,
        "total_urls": len(all_urls),
        "processed_urls": processed_urls,
        "new_urls_map": new_urls_map,
        "remaining_urls": remaining_urls,
        "file_content": file_info.get("content", "")
    }
    
    state_dir = os.path.join(os.path.expanduser("~"), ".xml_image_processor")
    os.makedirs(state_dir, exist_ok=True)
    state_file = os.path.join(state_dir, f"session_{session_id}.json")
    
    with open(state_file, "w") as f:
        json.dump(state, f)
    
    return state_file

def verify_state_consistency(state):
    """Weryfikuje sp√≥jno≈õƒá stanu i zwraca listƒô problem√≥w"""
    problems = []
    
    required_keys = ["session_id", "timestamp", "file_info", "processing_params", 
                    "total_urls", "processed_urls", "new_urls_map", "remaining_urls"]
    
    # Sprawd≈∫ wymagane klucze
    for key in required_keys:
        if key not in state:
            problems.append(f"Brak wymaganego klucza: {key}")
            
    # Sprawd≈∫ sp√≥jno≈õƒá liczby URL-i
    if "total_urls" in state and "processed_urls" in state and "remaining_urls" in state:
        total = state["total_urls"]
        processed = len(state["processed_urls"])
        remaining = len(state["remaining_urls"])
        
        if total != processed + remaining:
            problems.append(f"Niesp√≥jno≈õƒá liczby URL-i: total={total}, processed={processed}, remaining={remaining}")
            
    # Sprawd≈∫ sp√≥jno≈õƒá mapowania URL-i
    if "new_urls_map" in state and "processed_urls" in state:
        mapped_urls = set(state["new_urls_map"].keys())
        processed_urls = set(state["processed_urls"])
        
        if mapped_urls != processed_urls:
            problems.append("Niesp√≥jno≈õƒá miƒôdzy przetworzonymi URL-ami a mapowaniem")
            
    return problems

def load_processing_state(session_id=None):
    state_dir = os.path.join(os.path.expanduser("~"), ".xml_image_processor")
    if not os.path.exists(state_dir):
        return None
    
    if session_id:
        state_file = os.path.join(state_dir, f"session_{session_id}.json")
        if os.path.exists(state_file):
            try:
                with open(state_file, "r") as f:
                    state = json.load(f)
                    
                # Sprawd≈∫ sp√≥jno≈õƒá stanu
                problems = verify_state_consistency(state)
                if problems:
                    st.warning("‚ö†Ô∏è Wykryto problemy ze stanem sesji:")
                    for problem in problems:
                        st.warning(f"- {problem}")
                    
                    # Spr√≥buj wczytaƒá kopiƒô zapasowƒÖ
                    backup_file = state_file + ".bak"
                    if os.path.exists(backup_file):
                        with open(backup_file, "r") as f:
                            backup_state = json.load(f)
                        backup_problems = verify_state_consistency(backup_state)
                        if not backup_problems:
                            st.info("‚úÖ Wczytano kopiƒô zapasowƒÖ stanu")
                            return backup_state
                            
                return state
            except Exception as e:
                st.error(f"B≈ÇƒÖd podczas wczytywania stanu: {str(e)}")
                return None
        return None
    
    # Znajd≈∫ najnowszy plik
    state_files = [f for f in os.listdir(state_dir) if f.startswith("session_") and f.endswith(".json")]
    if not state_files:
        return None
    
    state_files.sort(key=lambda x: os.path.getmtime(os.path.join(state_dir, x)), reverse=True)
    with open(os.path.join(state_dir, state_files[0]), "r") as f:
        state = json.load(f)
        
    # Sprawd≈∫ sp√≥jno≈õƒá najnowszego stanu
    problems = verify_state_consistency(state)
    if problems:
        st.warning("‚ö†Ô∏è Wykryto problemy z najnowszym stanem:")
        for problem in problems:
            st.warning(f"- {problem}")
    
    return state

def list_saved_sessions():
    state_dir = os.path.join(os.path.expanduser("~"), ".xml_image_processor")
    if not os.path.exists(state_dir):
        return []
    
    sessions = []
    for filename in os.listdir(state_dir):
        if filename.startswith("session_") and filename.endswith(".json"):
            try:
                with open(os.path.join(state_dir, filename), "r") as f:
                    state = json.load(f)
                    progress = round(len(state["processed_urls"]) / state["total_urls"] * 100, 1) if state["total_urls"] > 0 else 0
                    sessions.append({
                        "session_id": state["session_id"],
                        "timestamp": state["timestamp"],
                        "file_info": state["file_info"]["name"],
                        "progress": f"{len(state['processed_urls'])}/{state['total_urls']}",
                        "percentage": progress
                    })
            except:
                pass
    
    sessions.sort(key=lambda x: x["timestamp"], reverse=True)
    return sessions

def resume_processing(state, temp_dir, ftp_settings, max_workers=5):
    remaining_urls = state["remaining_urls"]
    new_urls_map = state.get("new_urls_map", {})
    file_info = state["file_info"]
    processing_params = state["processing_params"]
    session_id = state["session_id"]
    
    # Przywr√≥ƒá parametry przetwarzania do sesji
    st.session_state.file_info = file_info
    st.session_state.processing_params = processing_params
    
    if not remaining_urls:
        st.success("Wszystkie URL-e zosta≈Çy ju≈º przetworzone.")
        return True
    
    progress_bar = st.progress(len(state["processed_urls"]) / state["total_urls"])
    status_text = st.empty()
    debug_area = st.empty()
    
    total_to_process = len(remaining_urls)
    status_text.text(f"Inicjalizacja wznawiania przetwarzania {len(remaining_urls)} pozosta≈Çych obraz√≥w...")
    
    # Pre-inicjalizacja FTP dla szybszego startu
    if debug_area:
        debug_area.info("Przygotowanie po≈ÇƒÖczenia FTP...")
    try:
        ftp_manager = FTPManager.get_instance(ftp_settings)
        ftp_manager.connect()
    except Exception as e:
        if debug_area:
            debug_area.warning(f"Problem z wstƒôpnym po≈ÇƒÖczeniem FTP: {str(e)}")
            
    # Mierzenie czasu wykonania
    start_time = time.time()
    
    # Funkcja aktualizacji postƒôpu z wiƒôkszƒÖ ilo≈õciƒÖ szczeg√≥≈Ç√≥w
    def update_progress(progress_value, processed, total_processed, total):
        # Oblicz ca≈Çkowity progres uwzglƒôdniajƒÖc ju≈º przetworzone obrazy
        overall_progress = (len(state["processed_urls"]) + total_processed) / state["total_urls"]
        progress_bar.progress(overall_progress)
        
        percent = int(progress_value*100)
        elapsed_time = time.time() - start_time
        
        if processed > 0 and elapsed_time > 0:
            speed = processed / elapsed_time
            remaining = (total - processed) / speed if speed > 0 else 0
            time_info = f" | ~{int(remaining/60)}m {int(remaining%60)}s pozosta≈Ço"
            speed_info = f" | {speed:.1f} obraz√≥w/s"
        else:
            time_info = ""
            speed_info = ""
            
        status_text.text(f"Przetwarzanie... {total_processed}/{total} ({percent}%){speed_info}{time_info}")
    
    # Use our improved parallel processing function
    batch_result, batch_downloaded, failed_urls = process_images_in_parallel(
        remaining_urls, 
        temp_dir, 
        ftp_settings,
        max_workers=max_workers,
        debug_container=debug_area,
        max_retries=3,
        progress_callback=update_progress,
        session_id=session_id
    )
    
    # Update state with new results
    new_urls_map.update(batch_result)
    processed_urls = state.get("processed_urls", []) + list(batch_result.keys())
    
    # Remove duplicates if any
    processed_urls = list(set(processed_urls))
    
    # Save the updated state
    save_processing_state(
        session_id, 
        list(set(state["remaining_urls"] + state["processed_urls"])),  # Remove duplicates 
        processed_urls, 
        new_urls_map, 
        file_info,
        processing_params
    )
    
    status_text.text(f"Zako≈Ñczono wznowione przetwarzanie. Pobrano i przes≈Çano {len(batch_downloaded)} obraz√≥w.")
    
    # Report on failed URLs
    if failed_urls:
        with st.expander(f"Nie uda≈Ço siƒô przetworzyƒá {len(failed_urls)} URL-i"):
            for fail in failed_urls[:20]:
                st.warning(f"{fail['url']}: {fail['error']}")
            if len(failed_urls) > 20:
                st.warning(f"... oraz {len(failed_urls) - 20} wiƒôcej.")
    
    # Aktualizacja pliku po wszystkich pobraniach
    if new_urls_map:
        file_type = file_info["type"]
        file_content = state.get("file_content", "")
        
        if not file_content:
            st.error("Brak tre≈õci pliku w zapisanym stanie - nie mo≈ºna zaktualizowaƒá pliku.")
            return False
        
        # Aktualizacja odpowiedniego typu pliku
        if file_type == "xml":
            xpath = processing_params.get("xpath", "")
            new_node_name = processing_params.get("new_node_name", "ftp")
            separator = processing_params.get("separator", ",")
            
            updated_content, error = update_xml_with_new_urls(
                file_content, xpath, new_urls_map, new_node_name, separator
            )
        else:
            column_name = processing_params.get("column_name", "")
            new_column_name = processing_params.get("new_column_name", "ftp_url")
            separator = processing_params.get("separator", ",")
            
            updated_content, error = update_csv_with_new_urls(
                file_content, column_name, new_urls_map, new_column_name, separator
            )
        
        if error:
            st.error(f"B≈ÇƒÖd podczas aktualizacji pliku: {error}")
            return False
        
        output_bytes = updated_content.encode(file_info["encoding"])
        st.success("Plik zosta≈Ç pomy≈õlnie zaktualizowany!")
        
        # Google Drive
        try:
            success, message = save_to_google_drive(output_bytes, file_info, new_urls_map)
            st.success(f"‚úÖ {message}") if success else st.warning(f"‚ö†Ô∏è {message}")
        except Exception as e:
            st.error(f"B≈ÇƒÖd Google Drive: {str(e)}")
        
        # Przycisk pobierania
        base_name = os.path.splitext(file_info["name"])[0]
        st.download_button(
            label=f"üìÅ Pobierz zaktualizowany plik",
            data=output_bytes,
            file_name=f"{base_name}_updated.{file_type}",
            mime="text/plain"
        )
        return True
    else:
        st.warning("Nie uda≈Ço siƒô przetworzyƒá ≈ºadnych nowych obraz√≥w.")
        return False

def reset_app_state():
    for key in list(st.session_state.keys()):
        if key not in ["authenticated", "ftp_settings"]:
            del st.session_state[key]
    initialize_session_state()
    st.rerun()

def main():
    st.set_page_config(page_title="Pobieranie zdjƒôƒá z XML/CSV", layout="centered")
    authenticate_user()
    initialize_session_state()

    st.title("Pobieranie zdjƒôƒá z XML/CSV")

    tab1, tab2, tab3 = st.tabs(["Pobieranie zdjƒôƒá", "Wzn√≥w przetwarzanie", "Pomoc"])

    with tab1:
        st.markdown("""
        To narzƒôdzie umo≈ºliwia pobieranie zdjƒôƒá z plik√≥w XML lub CSV i zapisywanie ich na serwerze FTP.
        Prze≈õlij plik, wska≈º lokalizacjƒô link√≥w do zdjƒôƒá, podaj dane FTP i pobierz zdjƒôcia.
        """)

        st.subheader("1. Wczytaj plik ≈∫r√≥d≈Çowy")
        uploaded_file = st.file_uploader("Wgraj plik XML lub CSV", type=["xml", "csv"])

        if uploaded_file:
            file_info, error = read_file_content(uploaded_file)
            if error:
                st.error(error)
            else:
                st.success(f"Wczytano plik: {file_info['name']} ({file_info['type'].upper()}, {file_info['encoding']})")
                st.session_state.file_info = file_info

        st.subheader("2. Konfiguracja pobierania zdjƒôƒá")

        if st.session_state.file_info:
            file_type = st.session_state.file_info["type"]

            if file_type == "xml":
                xpath = st.text_input("XPath do wƒôz≈Ça zawierajƒÖcego URL-e zdjƒôƒá", 
                                    placeholder="Np. //product/image lub //image/@url")
                new_node_name = st.text_input("Nazwa nowego wƒôz≈Ça dla link√≥w FTP", 
                                            placeholder="Np. ftp")
                st.session_state.processing_params["xpath"] = xpath
                st.session_state.processing_params["new_node_name"] = new_node_name
            else:
                column_name = st.text_input("Nazwa kolumny zawierajƒÖcej URL-e zdjƒôƒá", 
                                          placeholder="Np. image_url")
                new_column_name = st.text_input("Nazwa nowej kolumny dla link√≥w FTP", 
                                              placeholder="Np. ftp_image_url")
                st.session_state.processing_params["column_name"] = column_name
                st.session_state.processing_params["new_column_name"] = new_column_name

            separator = st.text_input("Separator URL-i (je≈õli w jednej kom√≥rce/wƒô≈∫le jest wiele link√≥w)", value=",")
            st.session_state.processing_params["separator"] = separator

        st.subheader("3. Konfiguracja serwera FTP")

        col1, col2 = st.columns(2)
        with col1:
            st.session_state.ftp_settings["host"] = st.text_input("Adres serwera FTP", 
                                                                 value=st.session_state.ftp_settings["host"])
            st.session_state.ftp_settings["port"] = st.number_input("Port", value=st.session_state.ftp_settings["port"],
                                                                   min_value=1, max_value=65535)
            st.session_state.ftp_settings["directory"] = st.text_input("Katalog docelowy", 
                                                                      value=st.session_state.ftp_settings["directory"])
            st.session_state.ftp_settings["http_path"] = st.text_input("≈öcie≈ºka HTTP do zdjƒôƒá",
                                                                      value=st.session_state.ftp_settings.get("http_path", ""),
                                                                      placeholder="https://example.com/images/")

        with col2:
            st.session_state.ftp_settings["username"] = st.text_input("Nazwa u≈ºytkownika", 
                                                                     value=st.session_state.ftp_settings["username"])
            st.session_state.ftp_settings["password"] = st.text_input("Has≈Ço", type="password", 
                                                                     value=st.session_state.ftp_settings["password"])

        st.subheader("4. Pobierz zdjƒôcia i prze≈õlij na FTP")
        
        if st.session_state.file_info:
            max_workers = st.slider(
                "Liczba r√≥wnoleg≈Çych proces√≥w pobierania", 
                min_value=FTP_CONFIG['MIN_WORKERS'],
                max_value=FTP_CONFIG['MAX_WORKERS'],
                value=FTP_CONFIG['DEFAULT_WORKERS'],
                help="Wy≈ºsza warto≈õƒá przyspieszy pobieranie, ale mo≈ºe obciƒÖ≈ºyƒá ≈ÇƒÖcze"
            )

        # --- Nowa sekcja: kontenery na status i kolejkƒô ---
        progress_bar = st.progress(0)
        status_text = st.empty()
        queue_info = st.empty()
        debug_area = st.empty()
        # ---

        if st.session_state.file_info and st.button("Pobierz zdjƒôcia i prze≈õlij na FTP"):
            try:
                # Generujemy nowe ID sesji
                session_id = f"{uuid.uuid4().hex}_{int(time.time())}"
                
                # Walidacja wej≈õƒá
                file_type = st.session_state.file_info["type"]
                file_content = st.session_state.file_info["content"]
                
                if file_type == "xml" and (not xpath or not xpath.strip() or not new_node_name or not new_node_name.strip()):
                    st.error("Podaj prawid≈Çowy XPath i nazwƒô nowego wƒôz≈Ça!")
                    return
                elif file_type == "csv" and (not column_name or not column_name.strip() or not new_column_name or not new_column_name.strip()):
                    st.error("Podaj prawid≈ÇowƒÖ nazwƒô kolumny i nazwƒô nowej kolumny!")
                    return
                    
                # Ekstrakcja URL-i
                if file_type == "xml" and xpath:
                    urls, error = extract_image_urls_from_xml(file_content, xpath, separator)
                elif file_type == "csv" and column_name:
                    urls, error = extract_image_urls_from_csv(file_content, column_name, separator)
                else:
                    urls, error = None, "Nie podano ≈õcie≈ºki XPath lub nazwy kolumny."
                    
                if error:
                    st.error(error)
                    return
                elif not urls:
                    st.warning("Nie znaleziono ≈ºadnych URL-i zdjƒôƒá.")
                    return
                
                # Zapisujemy poczƒÖtkowy stan
                save_processing_state(
                    session_id,
                    urls,
                    [],
                    {},
                    st.session_state.file_info,
                    st.session_state.processing_params
                )
                
                # Rozpocznij przetwarzanie
                with tempfile.TemporaryDirectory() as tmpdirname:
                    status_text.text(f"Inicjalizacja przetwarzania {len(urls)} obraz√≥w...")
                    queue_info.text("")
                    start_time = time.time()
                    st.session_state["last_logs"] = []
                    st.session_state["output_bytes"] = None
                    st.session_state["output_filetype"] = file_type
                    st.session_state["output_filename"] = st.session_state.file_info["name"]
                    st.session_state["output_log"] = None
                    st.session_state["output_new_urls_map"] = None

                    # ---
                    def progress_callback(progress, processed, uploaded, total):
                        percent = int(progress*100)
                        elapsed = time.time() - start_time
                        speed = uploaded/elapsed if elapsed > 0 else 0
                        remaining = (total-processed)/speed if speed > 0 else 0
                        status_text.text(f"Przetwarzanie... {processed}/{total} ({percent}%) | Przes≈Çano na FTP: {uploaded} | Szybko≈õƒá: {speed:.1f} obraz√≥w/s | ~{int(remaining/60)}m {int(remaining%60)}s pozosta≈Ço")
                        progress_bar.progress(progress)
                    # ---
                    def queue_callback(info):
                        queue_info.text(info)
                    # ---
                    # Ulepszona funkcja przetwarzania r√≥wnoleg≈Çego
                    new_urls_map, downloaded_images, failed_urls = process_images_in_parallel(
                        urls,
                        tmpdirname,
                        st.session_state.ftp_settings,
                        max_workers=max_workers,
                        debug_container=debug_area,
                        max_retries=3,
                        progress_callback=progress_callback,
                        session_id=session_id
                    )
                    # ---
                    progress_bar.progress(1.0)
                    status_text.text(f"Zako≈Ñczono przetwarzanie. Pobrano i przes≈Çano {len(new_urls_map)} z {len(urls)} obraz√≥w.")
                    
                    # Zapisz stan
                    processed_urls = list(new_urls_map.keys())
                    remaining_urls = [url for url in urls if url not in processed_urls]
                    save_processing_state(
                        session_id, urls, processed_urls, new_urls_map, 
                        st.session_state.file_info, st.session_state.processing_params
                    )
                    
                    # Raportuj b≈Çƒôdy
                    if failed_urls:
                        with st.expander(f"Nie uda≈Ço siƒô przetworzyƒá {len(failed_urls)} URL-i"):
                            for fail in failed_urls[:20]:
                                st.warning(f"{fail['url']}: {fail['error']}")
                            if len(failed_urls) > 20:
                                st.warning(f"... oraz {len(failed_urls) - 20} wiƒôcej.")
                    
                    # Aktualizacja pliku
                    if new_urls_map:
                        # Wyb√≥r odpowiedniej funkcji aktualizacji
                        if file_type == "xml":
                            updated_content, error = update_xml_with_new_urls(
                                file_content, xpath, new_urls_map, new_node_name, separator
                            )
                        else:
                            updated_content, error = update_csv_with_new_urls(
                                file_content, column_name, new_urls_map, new_column_name, separator
                            )

                        if error:
                            st.error(f"B≈ÇƒÖd podczas aktualizacji pliku: {error}")
                        else:
                            # Kodowanie wyniku
                            st.session_state.output_bytes = updated_content.encode(
                                st.session_state.file_info["encoding"]
                            )
                            st.success("Plik zosta≈Ç zaktualizowany o nowe linki FTP.")
                            
                            # Google Drive
                            try:
                                success, message = save_to_google_drive(
                                    st.session_state.output_bytes,
                                    st.session_state.file_info,
                                    new_urls_map
                                )
                                
                                if success:
                                    st.success(f"‚úÖ {message}")
                                else:
                                    st.warning(f"‚ö†Ô∏è {message}")
                                
                            except Exception as e:
                                st.error(f"B≈ÇƒÖd Google Drive: {str(e)}")

                            # Przycisk pobierania
                            original_name = st.session_state.file_info["name"]
                            base_name = os.path.splitext(original_name)[0]
                            st.download_button(
                                label="üìÅ Pobierz zaktualizowany plik",
                                data=st.session_state.output_bytes,
                                file_name=f"{base_name}_updated.{file_type}",
                                mime="text/plain"
                            )
                            
                            # Je≈õli sƒÖ nieudane URL-e, zaproponuj ponowienie
                            if failed_urls and len(failed_urls) > 0:
                                st.warning(f"Nie uda≈Ço siƒô przetworzyƒá {len(failed_urls)} obraz√≥w. Mo≈ºesz wznowiƒá przetwarzanie z zak≈Çadki 'Wzn√≥w przetwarzanie'.")

                    if st.button("Rozpocznij nowƒÖ operacjƒô"):
                        reset_app_state()

            except (KeyboardInterrupt, SystemExit, st.runtime.scriptrunner.StopException):
                st.warning("Przetwarzanie zosta≈Ço przerwane. Mo≈ºesz je wznowiƒá p√≥≈∫niej z zak≈Çadki 'Wzn√≥w przetwarzanie'.")
                return
            except Exception as e:
                st.error(f"WystƒÖpi≈Ç b≈ÇƒÖd: {str(e)}")
                st.info("Mo≈ºesz spr√≥bowaƒá wznowiƒá przetwarzanie z zak≈Çadki 'Wzn√≥w przetwarzanie'.")
                return

    with tab2:
        st.subheader("Wzn√≥w wcze≈õniej przerwane przetwarzanie")
        
        saved_sessions = list_saved_sessions()
        if not saved_sessions:
            st.info("Nie znaleziono zapisanych sesji przetwarzania.")
        else:
            st.write("Wybierz sesjƒô do wznowienia:")
            
            # Tabela sesji
            sessions_df = pd.DataFrame(saved_sessions)
            if not sessions_df.empty:
                st.dataframe(sessions_df[["timestamp", "file_info", "progress", "percentage"]])
                
                # Wyb√≥r sesji
                selected_session_id = st.selectbox(
                    "Wybierz ID sesji do wznowienia:", 
                    options=[s["session_id"] for s in saved_sessions],
                    format_func=lambda x: f"{next((s['timestamp'] for s in saved_sessions if s['session_id'] == x), '')} - {next((s['file_info'] for s in saved_sessions if s['session_id'] == x), '')}"
                )
                
                max_workers_resume = st.slider(
                    "Liczba r√≥wnoleg≈Çych proces√≥w", 
                    min_value=FTP_CONFIG['MIN_WORKERS'],
                    max_value=FTP_CONFIG['MAX_WORKERS'],
                    value=FTP_CONFIG['DEFAULT_WORKERS']
                )
                
                if st.button("Wzn√≥w przetwarzanie"):
                    state = load_processing_state(selected_session_id)
                    if state:
                        with tempfile.TemporaryDirectory() as tmpdirname:
                            resume_processing(state, tmpdirname, st.session_state.ftp_settings, max_workers=max_workers_resume)
                    else:
                        st.error("Nie uda≈Ço siƒô za≈Çadowaƒá stanu sesji.")

    with tab3:
        st.markdown("""
        ### Jak korzystaƒá z aplikacji

        1. **Wgraj plik XML lub CSV** - aplikacja automatycznie wykryje kodowanie
        2. **Skonfiguruj pobieranie zdjƒôƒá**:
           - Dla XML: Podaj XPath do wƒôz≈Ça zawierajƒÖcego URL-e zdjƒôƒá
           - Dla CSV: Podaj nazwƒô kolumny zawierajƒÖcej URL-e zdjƒôƒá
           - Okre≈õl separator, je≈õli w jednej kom√≥rce/wƒô≈∫le znajduje siƒô wiele URL-i
        3. **Skonfiguruj serwer FTP** - podaj dane dostƒôpowe do serwera FTP
           - Podaj ≈õcie≈ºkƒô HTTP, pod kt√≥rƒÖ bƒôdƒÖ dostƒôpne zdjƒôcia (np. https://example.com/images/)
        4. **Pobierz zdjƒôcia i prze≈õlij na FTP** - aplikacja pobierze zdjƒôcia i prze≈õle je na serwer FTP
           - Mo≈ºesz dostosowaƒá liczbƒô r√≥wnoleg≈Çych proces√≥w pobierania dla przyspieszenia procesu
        5. **Pobierz zaktualizowany plik** - plik ≈∫r√≥d≈Çowy zostanie zaktualizowany o nowe linki HTTP/FTP
        6. **Wznawianie przerwanego procesu**:
           - W przypadku b≈Çƒôdu lub przerwania procesu, przejd≈∫ do zak≈Çadki "Wzn√≥w przetwarzanie"
           - Wybierz odpowiedniƒÖ sesjƒô i kontynuuj pobieranie od miejsca przerwania

        ### Przyk≈Çady konfiguracji

        #### XML

        - XPath: //product/image
        - Nazwa nowego wƒôz≈Ça: ftp
        - ≈öcie≈ºka HTTP: https://example.com/images/

        #### CSV

        - Nazwa kolumny: image_url
        - Nazwa nowej kolumny: ftp_image_url
        - ≈öcie≈ºka HTTP: https://example.com/images/

        ### Obs≈Çugiwane formaty

        - **XML** - pliki XML z linkami do zdjƒôƒá w okre≈õlonych wƒôz≈Çach
        - **CSV** - pliki CSV z linkami do zdjƒôƒá w okre≈õlonych kolumnach
        """)

if __name__ == "__main__":
    main()
